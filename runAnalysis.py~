import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sys
from sklearn import tree, svm
from sklearn.ensemble import AdaBoostClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import linear_model, datasets, metrics
from sklearn.neural_network import BernoulliRBM
from sklearn.pipeline import Pipeline
import HiggsBosonCompetition_AMSMetric_rev1 as ams

# Need an adapter to give the gradientboostclassifier a decisiontree as a parameter
# https://stackoverflow.com/questions/17454139/gradientboostingclassifier-with-a-baseestimator-in-scikit-learn/19679862#19679862
class BaseTree:
    def __init__(self, est):
        self.est = est
    def predict(self, X):
        return self.est.predict_proba(X)[:,1][:,np.newaxis]
    def fit(self, X, y):
        self.est.fit(X, y)

# write out the solution file in the correct format
def solnFile(fname, cls, eventid, bkg=None):
    f = open('/media/win/higgscomp/'+fname+'.out','w')
    # need to figure out the rankorder still
   
    f.write('EventId,RankOrder,Class\n')
    count = 1
    for x in xrange(0,len(cls)):
        f.write(str(eventid[x])+','+str(count)+','+str(cls[x])+'\n')
        count+=1
    if bkg:
        for x,r in bkg.iterrows():
            f.write(str(r['EventId'])+','+str(count)+',b\n')
            count+=1
    f.close()

# list of all the variables
cols = ['EventId', 'DER_mass_MMC', 'DER_mass_transverse_met_lep',
       'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet',
       'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep',
       'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau',
       'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt',
       'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta',
       'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet',
       'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta',
       'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',
       'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi',
       'PRI_jet_all_pt', 'Weight', 'Label']
# These are the variables we want to train on
train_input = ['DER_mass_MMC', 'DER_mass_transverse_met_lep',
       'DER_mass_vis', 'DER_pt_h', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep',
       'DER_sum_pt', 'DER_pt_ratio_lep_tau',
       'DER_met_phi_centrality', 'DER_lep_eta_centrality', 
       'PRI_jet_num']


#tr = pd.read_csv('training.csv')
tr = pd.read_csv('training_orig.csv')
#test = pd.read_csv('testing.csv')
test = pd.read_csv('test_orig.csv')

# How much does it change the results if we apply presel cuts on training and test, vs test ONLY?

# Apply some basic cuts to remove backgrounds
# all failed events are already background - need to write this to file or save it somewhere
# tr=tr[(tr.varname > something) & (tr.varname < something)]
# mtw cut of 70 gev
sigtr=tr#[(tr.DER_mass_transverse_met_lep < 70) & (tr.DER_mass_vis > 30) & (tr.DER_mass_vis < 140) & ((tr.DER_mass_transverse_met_lep > 40) | (tr.DER_mass_MMC > 110))]
#bkgtr=tr[(tr.DER_mass_transverse_met_lep > 70) | (tr.DER_mass_vis < 30) | (tr.DER_mass_vis > 140) | ((tr.DER_mass_transverse_met_lep < 40) & (tr.DER_mass_MMC < 110))]


sigtest=test#[(test.DER_mass_transverse_met_lep < 70) & (test.DER_mass_vis > 30) & (test.DER_mass_vis < 140) & ((test.DER_mass_transverse_met_lep > 40) | (test.DER_mass_MMC > 110))]
#bkgtest=test[(test.DER_mass_transverse_met_lep > 70) | (test.DER_mass_vis < 30) | (test.DER_mass_vis > 140) | ((test.DER_mass_transverse_met_lep < 40) & (test.DER_mass_MMC < 110))]

# try a cut on the vis mass?
#sig = pd.concat([sig,tr[(tr.DER_mass_vis > 30) & (tr.DER_mass_vis < 140)]])
# remove z->tautau enriched control region
#sig = pd.concat([sig,tr[(tr.DER_mass_transverse_met_lep > 40) & (tr.DER_mass_mmc > 110)]])



# DecisionTreeClassifier http://scikit-learn.org/stable/modules/tree.html#tree-classification
def runDTree(depth, filename):
    clf = tree.DecisionTreeClassifier(max_depth=depth)
    print "DecisionTreeClassifier training"
    clf = clf.fit(sigtr[train_input].values, sigtr['Label'].values)
    print "DecisionTreeClassifier testing"
    clf_pred = clf.predict(sigtest[train_input].values)
    solnFile('dtc_'+filename,clf_pred,sigtest['EventId'].values)#
    print "DecisionTreeClassifier finished"

# Ensemble methods http://scikit-learn.org/stable/modules/ensemble.html
# AdaBoostClassifier
def runAdaBoost(depth, n_est, filename, lrn_rate=1.0):
    #ada = AdaBoostClassifier(n_estimators=100)
    ada = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=depth),
                             algorithm="SAMME",
                             n_estimators=n_est)#,n_jobs=4)
    print "AdaBoost training"
    ada.fit(sigtr[train_input].values,sigtr['Label'].values)
    print "AdaBoost testing"
    ada_pred = ada.predict(sigtest[train_input].values)
    solnFile('ada_'+filename,ada_pred,sigtest['EventId'].values)#
    print "AdaBoost finished"

# http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html#example-ensemble-plot-adaboost-multiclass-py
def runAdaReal(depth, n_est, filename, lrn_rate=1.0):
    bdt_real = AdaBoostClassifier(
        tree.DecisionTreeClassifier(max_depth=depth),
        n_estimators=n_est,
        learning_rate=lrn_rate)
    print "AdaBoostReal training"
    bdt_real.fit(sigtr[train_input].values,sigtr['Label'].values)
    print "AdaBoostReal testing"
    bdt_real_pred = bdt_real.predict(sigtest[train_input].values)
    solnFile('bdt_real_'+filename,bdt_real_pred,sigtest['EventId'].values)#
    print "AdaBoostReal finished"

# GradientBoostingClassifier
def runGDB(depth, n_est, filename, lrn_rate=1.0):
    print "GDB training"
    gdb = GradientBoostingClassifier(init=BaseTree(tree.DecisionTreeClassifier(max_depth=depth)),n_estimators=n_est, learning_rate=lrn_rate, random_state=0).fit(sigtr[train_input].values, sigtr['Label'].values)
    print "GDB testing"
    gdb_pred = gdb.predict(sigtest[train_input].values)
    solnFile('gdb_'+filename,gdb_pred,sigtest['EventId'].values)#
    print "GDB finished"

# ANN - http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.html#example-plot-rbm-logistic-classification-py
def runRBM(iters, lrn_rate, logistic_c_val, logistic_c_val2, n_comp, filename):
    logistic = linear_model.LogisticRegression()
    rbm = BernoulliRBM(random_state=0, verbose=True)
    
    classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])

    ###############################################################################
    # Training

    # Hyper-parameters. These were set by cross-validation,
    # using a GridSearchCV. Here we are not performing cross-validation to
    # save time.
    rbm.learning_rate = lrn_rate #0.10#0.06
    rbm.n_iter = iters #20
    # More components tend to give better prediction performance, but larger
    # fitting time
    rbm.n_components = n_comp # 250
    logistic.C = logistic_c_val #6000.0

    # Training RBM-Logistic Pipeline
    classifier.fit(sigtr[train_input].values, sigtr['Label'].values)

    # Training Logistic regression
    logistic_classifier = linear_model.LogisticRegression(C=logistic_c_val2)#100.0
    logistic_classifier.fit(sigtr[train_input].values, sigtr['Label'].values)

    ###############################################################################
    # Evaluation
    clsnn_pred=classifier.predict(sigtest[train_input].values)
    solnFile('clsnn_'+filename,clsnn_pred,sigtest['EventId'].values)#,bkgtest)
    
    log_cls_pred = logistic_classifier.predict(sigtest[train_input].values)
    solnFile('lognn_'+filename,log_cls_pred,sigtest['EventId'].values)#,bkgtest)
    
    logistic_classifier_tx = linear_model.LogisticRegression(C=logistic_c_val2)
    logistic_classifier_tx.fit_transform(sigtr[train_input].values, sigtr['Label'].values)
    log_cls_tx_pred = logistic_classifier_tx.predict(sigtest[train_input].values)
    solnFile('lognntx_'+filename,log_cls_tx_pred,sigtest['EventId'].values)#,bkgtest)

def maximiseScore():
    solutionFile = "solutions.csv"
    # parameters to vary - n_estimators, max_depth, learning_rate
    # vary input variables
    # vary testing/ training with/without background cuts
    ams_best = 0.0
    ams_prev = 0.0
    nEvents = 125000
    while not optimal:
        runAdaBoost(x) # high
        runAdaBoost(x) # low
        submissionFileUp=combOfParams+'up.out'
        submissionFileDo=combOfParams+'do.out'
        ams_up = AMS_metric(solutionFile, submissionFileUp, nEvents)
        ams_do = AMS_metric(solutionFile, submissionFileDo, nEvents)
        if ams_up >= ams_best:
            ams_best = ams_up
            # store parameters used
        if ams_up < ams_do:
            up = (h+l)/2
            # change things - we want to run higher and lower.... this recursion might destroy us
            varsUp = somecombinationofnewupperlimit
        else:
            do = (h+l)/2
            varsDo = blah
            # store parameters used
        if up-low <= 0:
            optimal = True
        # perhaps stop if the last x iterations have failed to produce better results??
        ams_prev = max(ams_up,ams_do)
